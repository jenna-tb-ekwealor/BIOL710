---
title: "Foundations for statistical inference - Sampling distributions"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE,message=FALSE}
library(BIOL710AdvBiometry)
library(learnr)
library(learnrhash)
library(tidyverse)
library(gradethis)
library(openintro)
library(infer)
tutorial_options(
  # use gradethis for checking
  exercise.checker = gradethis::grade_learnr
  )
knitr::opts_chunk$set(echo = FALSE)

global_monitor <- tibble(
  scientist_work = c(rep("Benefits", 80000), rep("Doesn't benefit", 20000))
)
```

## Getting started

In this lab, you will investigate the ways in which the statistics from a random 
sample of data can serve as point estimates for population parameters. We're 
interested in formulating a *sampling distribution* of our estimate in order 
to learn about the properties of the estimate, such as its distribution.


### Load packages

In this lab, we will explore and visualize the data using the **tidyverse** suite of packages. 
We will also use the **infer** package for resampling.

Let's load the packages.

```{r load-packages, exercise = T}
library(tidyverse)
library(openintro)
library(infer)
```

### Random samples

We will take some random samples and build sampling distributions
in this lab. This means that each time you rerun the sampling code, you will get different answers. This is not ideal as your answers would have to change each time you reran code. By "setting a seed" at the start of your lab you tell the random number generator where to start in its sequence of random numbers (so you'll get the same answer each time you rerun the code in the same order). You can think of the number you pick for `set.seed()` as a bookmark for the sampling algorithm.

Talking more about this is beyond the scope of the class, but if you are interested, I point you to Kellie Ottoboni's cool work on [sampling in R](http://www.kellieottoboni.com/posts/2019/01/random-problems-with-r/).

```{r set-seed, exercise = T}
set.seed(818155)
```

## The data

A 2019 Gallup report states the following:

>The premise that scientific progress benefits people has been embodied in discoveries throughout the ages -- from the development of vaccinations to the explosion of technology in the past few decades, resulting in billions of supercomputers now resting in the hands and pockets of people worldwide. Still, not everyone around the world feels science benefits them personally. 
>
>**Source:** [World Science Day: Is Knowledge Power?](https://news.gallup.com/opinion/gallup/268121/world-science-day-knowledge-power.aspx)

The Wellcome Global Monitor finds that 20% of people globally do not believe that the work scientists do benefits people like them.
In this lab, you will assume this 20% is a true population proportion and learn about how sample proportions can vary from sample to sample by taking smaller samples from the population. 
We will first create our population assuming a population size of 100,000. 
This means 20,000 (20%) of the population think the work scientists do does not 
benefit them personally and the remaining 80,000 think it does.

```{r, data, exercise = T}
global_monitor <- tibble(
  scientist_work = c(rep("Benefits", 80000), rep("Doesn't benefit", 20000))
)
```

The `tibble()` function makes a tidy data frame. The name of the data frame is `global_monitor` and the name of the variable that contains responses to the question *"Do you believe that the work scientists do benefit people like you?"* is `scientist_work`. To populate this variable we `c`ombine the following two groups: `rep`eat "Benefits 80,000 times and `rep`eat ``Doesn't benefit" 20,000 times.

We can quickly visualize the distribution of these responses using a bar plot. Note that `coord_flip()` makes the bars go left to right instead of top to bottom to save space on the page.

```{r bar-plot-pop, exercise = T}
ggplot(global_monitor, aes(x = scientist_work)) +
  geom_bar() +
  labs(
    x = "", y = "",
    title = "Do you believe that the work scientists do benefit people like you?"
  ) +
  coord_flip() 
```

We can also obtain summary statistics to confirm we constructed the data frame correctly. `count` is a shortcut to `group_by(scientist_work) %>% summarise(n = n())`

```{r summ-stat-pop, exercise = T}
global_monitor %>%
  count(scientist_work) 

global_monitor %>%
  count(scientist_work) %>%
  mutate(p = n /sum(n))
```

## The unknown 

In this lab, you have access to the entire population, but this is rarely the case in real life. 
Gathering information on an entire population is often extremely costly or impossible. 
Because of this, we often take a sample of the population and use that to understand the properties of the population.

If you are interested in estimating the proportion of people who don't think the work scientists do benefits them, you can use the `slice_sample` command to survey the population.

```{r samp1, exercise = T}
samp1 <- global_monitor %>%
  slice_sample(n = 50)
```

This command collects a simple random sample of size 50 from the `global_monitor` dataset, and assigns the result to `samp1`. 
This is similar to randomly drawing names from a hat that contains the names of all in the population.
Working with these 50 names is considerably simpler than working with all 100,000 people in the population.

Compare the proportions in the sample to the proportions in the population. Label the proportions in the sample `p_hat` to signal that it is a sample statistic rather than the true population parameter `p`. 

```{r comparesamp-setup}
samp1 <- global_monitor %>%
  slice_sample(n = 50)
```

```{r comparesamp, exercise = TRUE}
global_monitor %>%
  count(scientist_work) %>%
  mutate(p = n /sum(n))

___ %>% 
  count(___) %>%
  mutate(___ = n/sum(n))
```

```{r comparesamp-solution}
global_monitor %>%
  count(scientist_work) %>%
  mutate(p = n /sum(n))

samp1 %>% 
  count(scientist_work) %>%
  mutate(p_hat = n/sum(n))

```

```{r comparesamp-check}
gradethis::grade_code()
```

If you're interested in estimating the proportion of all people who do not believe that the work scientists do benefits them, but you do not have access to the population data, your best single guess is the sample mean that you calculated above.

## More sampling

Depending on which 50 people you selected, your estimate could be a bit above 
or a bit below the true population proportion. 
In general, though, the sample proportion turns out to be a pretty good estimate of the true population proportion, and you were able to get it by sampling less than 1% of the population.

Take a second sample, also of size 50, and call it `samp2`. 

```{r anothersamp, exercise = TRUE}
___ <- global_monitor %>%
  slice_sample(n = ___)
```

```{r anothersamp-solution}
samp2 <- global_monitor %>%
  slice_sample(n = 50)
```

```{r anothersamp-check}
# check code
gradethis::grade_code()
```

How does the sample proportion of `samp2` compare with that of `samp1`? 

```{r anothersampcompare-setup}
samp2 <- global_monitor %>%
  slice_sample(n = 50)
```

```{r anothersampcompare, exercise = TRUE}
___ %>% 
  ___(scientist_work) %>%
  ___(p_hat = n/sum(n))
```


```{r anothersampcompare-solution}
samp2 %>% 
  count(scientist_work) %>%
  mutate(p_hat = n/sum(n))
```

```{r anothersampcompare-check}
# check code
gradethis::grade_code()
```

```{r samplesize}
question("Suppose we took two 
    more samples, one of size 100 and one of size 1000. Which would you think 
    would provide a more accurate estimate of the population proportion?",
    answer("The sample of size 100"),
    answer("The sample of size 1000", correct = T),
    answer("They both would provide the same level of accuracy"),
    allow_retry = TRUE,
    random_answer_order = F
  )
```

## Sampling distribution

Not surprisingly, every time you take another random sample, you might get a different sample proportion. 
It's useful to get a sense of just how much variability you should expect when estimating the population mean this way. 
The distribution of sample proportions, called the *sampling distribution (of the proportion)*, can help you understand this variability. 
In this lab, because you have access to the population, you can build up the sampling distribution for the sample proportion by repeating the above steps many times. 
Here, we use R to take 15,000 different samples of size 50 from the population, calculate the proportion of responses in each sample, filter for only the *Doesn't benefit* responses, and store each result in a vector called `sample_props50`. 
Note that we specify that `replace = TRUE` since sampling distributions are constructed by sampling with replacement. We can filter the estimates that correspond to "Doesn't benefit" only as that is our main focus.

```{r iterate, exercise = T}
sample_props50 <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 15000, replace = TRUE) %>%
                    count(scientist_work) 

head(sample_props50)

sample_props50 <- sample_props50 %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

head(sample_props50)
```


And we can visualize the distribution of these proportions with a histogram.

```{r showres-setup}
sample_props50 <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 15000, replace = TRUE) %>%
                    count(scientist_work) 

sample_props50 <- sample_props50 %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")
```

```{r showres, exercise = T}
ggplot(data = sample_props50, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02) +
  labs(
    x = "p_hat (Doesn't benefit)",
    title = "Sampling distribution of p_hat",
    subtitle = "Sample size = 50, Number of samples = 15000"
  )

```


### What is a sampling distribution?

The idea behind the `rep_sample_n` function is *repetition*. 
Earlier, you took a single sample of size `n` (50) from the population of all people in the population. 
With this new function, you can repeat this sampling procedure `rep` times in order to build a distribution of a series of sample statistics, which is called the **sampling distribution**. 

For some insight into how functions and names change over time, here is some [context](https://github.com/tidymodels/infer/issues/337) on the function naming. `slice_sample()` took over for an older function `sample_n()`. In the next update of the `infer` package, `rep_sample_n()` will become `rep_slice_sample()`.

Note that in practice one rarely gets to build true sampling distributions, because one rarely has access to data from the entire population. 

Without the `rep_sample_n` function, this would be painful. 
We would have to manually run the following code 15,000 times 

```{r sample-code, exercise = T}
sample_props50 <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 15000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")
```

as well as store the resulting sample proportions each time in a separate vector.

Note that for each of the 15,000 times we computed a proportion, we did so from a **different** sample!

To make sure you understand how sampling distributions are built, and exactly
    what the `rep_sample_n` function does, try modifying the code to create a
    sampling distribution of **25 sample proportions** from **samples of size 10**, 
    and put them in a data frame named `sample_props_small`. Print the output. 
  
    
```{r repsamp, exercise = TRUE}
sample_props___ <- global_monitor %>%
                    rep_sample_n(size = ___, reps = ___, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")
___
```


```{r repsamp-solution}
sample_props_small <- global_monitor %>%
                    rep_sample_n(size = 10, reps = 25, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")
sample_props_small
```

```{r repsamp-check}
# check code
gradethis::grade_code()
```

```{r obs}
question("How many observations are there in this object called `sample_props_small`? Consider what each observation represents.",
    answer("10"),
    answer("20"),
    answer("25", correct = T),
    answer("50"),
    allow_retry = TRUE,
    random_answer_order = F
  )
```

    
## Sample size 

Mechanics aside, let's return to the reason we used the `rep_sample_n` function: to compute a sampling distribution, specifically, the sampling distribution of the proportions from samples of 50 people. 

```{r hist-setup}
sample_props50 <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 15000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")
```

```{r hist, exercise = T}
ggplot(data = sample_props50, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02)
```

The sampling distribution that you computed tells you much about estimating the true proportion of people who think that the work scientists do doesn't benefit them. 
Because the sample proportion is an unbiased estimator, the sampling distribution is centered at the true population proportion, and the spread of the distribution indicates how much variability is incurred by sampling only 50 people at a time from the population.

In the remainder of this section, you will work on getting a sense of the effect that sample size has on your sampling distribution.

```{r diffsampsize, exercise = T}
sample_props_10 <- global_monitor %>%
                    rep_sample_n(size = 10, reps = 5000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

sample_props_50 <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

sample_props_100 <- global_monitor %>%
                    rep_sample_n(size = 100, reps = 5000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")
```

```{r diffsampsize2-setup}
sample_props_10 <- global_monitor %>%
                    rep_sample_n(size = 10, reps = 5000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

sample_props_50 <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

sample_props_100 <- global_monitor %>%
                    rep_sample_n(size = 100, reps = 5000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")
```

```{r diffsampsize2, exercise = T}
ggplot(data = sample_props_10, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02) + xlim(0,1)
ggplot(data = sample_props_50, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02) + xlim(0,1)
ggplot(data = sample_props_100, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02) + xlim(0,1)
```


```{r meanchange}
question("How does the mean change as the sample size increases?",
    answer("gets smaller"),
    answer("gets larger"),
    answer("stays about the same", correct = T),
    allow_retry = TRUE,
    random_answer_order = F
  )
```

```{r sdchange}
question("How does the standard error change as the sample size increases?",
    answer("gets smaller", correct = T),
    answer("gets larger"),
    answer("stays about the same"),
    allow_retry = TRUE,
    random_answer_order = F
  )
```

```{r shapechange}
question("How does the shape of the distribution change as the sample size increases?",
    answer("gets rougher"),
    answer("gets smoother"),
    answer("stays about the same", correct = T),
    allow_retry = TRUE,
    random_answer_order = F
  )
```

## Number of replicates

Now we'll keep the sample size at 50 and see what happens when we change the number of replicates.

```{r diffrepsize, exercise = T}
sample_props_50_small <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 50, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

sample_props_50_medium <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 500, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

sample_props_50_large <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

```

```{r diffrepsize2-setup}
sample_props_50_small <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 50, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

sample_props_50_medium <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 500, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")

sample_props_50_large <- global_monitor %>%
                    rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
                    count(scientist_work) %>%
                    mutate(p_hat = n /sum(n)) %>%
                    filter(scientist_work == "Doesn't benefit")
```

```{r diffrepsize2, exercise = T}
ggplot(data = sample_props_50_small, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02) + xlim(0,1)

ggplot(data = sample_props_50_medium, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02) + xlim(0,1)

ggplot(data = sample_props_50_large, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02) + xlim(0,1)

```

```{r meanchange2}
question("How does the mean change as the number of replicates increases?",
    answer("gets smaller"),
    answer("gets larger"),
    answer("stays about the same", correct = T),
    allow_retry = TRUE,
    random_answer_order = F
  )
```

```{r sdchange2}
question("How does the standard error change as the number of replicates increases?",
    answer("gets smaller"),
    answer("gets larger"),
    answer("stays about the same", correct = T),
    allow_retry = TRUE,
    random_answer_order = F
  )
```

```{r shapechange2}
question("How does the shape of the distribution change as the number of replicates increases?",
    answer("gets rougher"),
    answer("gets smoother", correct = T),
    answer("stays about the same"),
    allow_retry = TRUE,
    random_answer_order = F
  )
```

## Submit checkpoint

```{r context="server"}
learnrhash::encoder_logic(strip_output = T)
```

```{r encode, echo=FALSE}
learnrhash::encoder_ui(
  ui_before = div(strong("Submit your hash in the form below."), br(), br()),
  ui_after  = learnrhash::iframe_ui(
    src = "https://docs.google.com/forms/d/e/1FAIpQLSfKAR9MMxfAR8EWsCi-U2Ow4SjsejLlAUba14FBsch-gox0Gg/viewform?usp=sf_link",
    width="900px", height= "1000px"
  )
)
```

* * *

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a> then was adapted and learnr-ified by Sara Stoudt, and further adapted and biologified by Jenna Ekwealor.
